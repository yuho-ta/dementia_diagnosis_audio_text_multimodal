# =============================
# ãƒã‚¤ã‚ºè¿½åŠ ä»˜ãwav2vecç‰¹å¾´é‡åˆ†é¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# - ãƒã‚¤ã‚ºè¿½åŠ ä»˜ãwav2vecç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ã¦åˆ†é¡
# - å„ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ã”ã¨ã®æ€§èƒ½æ¯”è¼ƒ
# - ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹è©•ä¾¡
# - çµæœã®å¯è¦–åŒ–ã¨ä¿å­˜
# - è¢«é¨“è€…IDãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²ï¼ˆåŒã˜è¢«é¨“è€…ã®ãƒ‡ãƒ¼ã‚¿ãŒtrain/testã«æ··åœ¨ã—ãªã„ï¼‰
# =============================

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from datetime import datetime
import json
from tqdm import tqdm
import wandb
import argparse
import sys
import re

# ãƒ­ã‚°è¨­å®š
log_filename = f"noise_augmented_classifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

import yaml

# å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š
def set_seed(seed=42):
    """å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # ãƒãƒ«ãƒGPUã®å ´åˆ
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š
set_seed(42)

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
config_path = os.path.join('configs', 'noise_augmented_wav2vec.yaml')
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# ãƒ‘ã‚¹è¨­å®š
features_path = os.path.join('dataset', 'diagnosis', 'train', 'noise_augmented_features')
output_path = os.path.join('results', 'noise_augmented_classification')
os.makedirs(output_path, exist_ok=True)

# è¨ºæ–­ã‚«ãƒ†ã‚´ãƒª
diagnosis = ['ad', 'cn']
label_mapping = {'ad': 1, 'cn': 0}

# åˆ†é¡è¨­å®š
classification_config = config['classification']

# wandbåˆæœŸåŒ–ã¯å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§è¡Œã†ãŸã‚ã€ã“ã“ã§ã¯è¡Œã‚ãªã„

def extract_subject_id_from_filename(filename):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç•ªå·ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆä¾‹ï¼š714-0_original.pt â†’ 714-0ï¼‰"""
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç•ªå·éƒ¨åˆ†ã‚’æŠ½å‡º
    match = re.match(r'(\d+-\d+)', filename)
    if match:
        return match.group(1)
    return None

def extract_par_id_from_cha_file(cha_file_path):
    """CHAãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰PARã®IDã‚’æŠ½å‡ºã™ã‚‹"""
    try:
        with open(cha_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # @IDè¡Œã§PARã®IDã‚’æ¢ã™
        par_id_match = re.search(r'@ID:\s*eng\|Pitt\|PAR\|.*', content)
        if par_id_match:
            return par_id_match.group(0)
        return None
    except Exception as e:
        logger.warning(f"ã‚¨ãƒ©ãƒ¼: {cha_file_path} ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {e}")
        return None

def get_subject_id_from_cha_file(uid, diagno):
    """UIDã¨è¨ºæ–­ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰CHAãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã—ã¦PAR IDã‚’æŠ½å‡ºã™ã‚‹"""
    # CHAãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    cha_file_path = os.path.join('dataset', 'diagnosis', 'train', 'segmentation', diagno, f"{uid}.cha")
    
    if os.path.exists(cha_file_path):
        par_id = extract_par_id_from_cha_file(cha_file_path)
        if par_id:
            # PAR IDã‹ã‚‰è¢«é¨“è€…IDéƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆä¾‹ï¼š@ID: eng|Pitt|PAR|59;|male|ProbableAD||Participant|11|| â†’ 59ï¼‰
            par_id_match = re.search(r'@ID:\s*eng\|Pitt\|PAR\|(\d+);', par_id)
            if par_id_match:
                return par_id_match.group(1)
    
    # CHAãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã‚„PAR IDãŒæŠ½å‡ºã§ããªã„å ´åˆã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æŠ½å‡º
    logger.warning(f"CHA file not found or PAR ID extraction failed for {uid}, falling back to filename extraction")
    return uid.split('-')[0] if '-' in uid else uid

class NoiseAugmentedDataset(Dataset):
    """ãƒã‚¤ã‚ºè¿½åŠ ä»˜ãç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆè¢«é¨“è€…IDãƒ™ãƒ¼ã‚¹åˆ†å‰²å¯¾å¿œï¼‰"""
    
    def __init__(self, features_path, noise_type='original'):
        self.features_path = features_path
        self.noise_type = noise_type
        self.data = []
        self.labels = []
        self.subject_ids = []  # è¢«é¨“è€…IDã‚’è¿½åŠ 
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        for diagno in diagnosis:
            diagno_path = os.path.join(features_path, diagno)
            if not os.path.exists(diagno_path):
                continue
                
            label = label_mapping[diagno]
            
            # æŒ‡å®šã•ã‚ŒãŸãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            for file in os.listdir(diagno_path):
                if file.endswith(f'_{noise_type}.pt'):
                    file_path = os.path.join(diagno_path, file)
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰UIDã‚’æŠ½å‡º
                    uid = extract_subject_id_from_filename(file)
                    
                    if uid:  # UIDãŒæŠ½å‡ºã§ããŸå ´åˆã®ã¿è¿½åŠ 
                        # CHAãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¢«é¨“è€…IDã‚’æŠ½å‡º
                        subject_id = get_subject_id_from_cha_file(uid, diagno)
                        
                        self.data.append(file_path)
                        self.labels.append(label)
                        self.subject_ids.append(subject_id)
        
        logger.info(f"Loaded {len(self.data)} samples for noise type: {noise_type}")
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¨ˆç®—
        class_counts = {}
        for label in self.labels:
            class_counts[label] = class_counts.get(label, 0) + 1
        logger.info(f"Class distribution: {class_counts}")
        
        # è¢«é¨“è€…IDã®çµ±è¨ˆæƒ…å ±
        unique_subjects = len(set(self.subject_ids))
        logger.info(f"Unique subjects: {unique_subjects}")
        
        # è¢«é¨“è€…ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ†å¸ƒ
        subject_counts = {}
        for subject_id in self.subject_ids:
            subject_counts[subject_id] = subject_counts.get(subject_id, 0) + 1
        
        logger.info(f"Subject sample distribution: min={min(subject_counts.values())}, max={max(subject_counts.values())}, mean={np.mean(list(subject_counts.values())):.2f}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ï¼ˆå…ƒã®é•·ã•ã®ã¾ã¾ï¼‰
        features = torch.load(self.data[idx])
        
        # ãƒã‚¹ã‚¯ã‚’ä½œæˆï¼ˆã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚’æ¤œå‡ºï¼‰
        mask = (features.sum(dim=-1) == 0)  # (time_steps,)
        
        return features.float(), self.labels[idx], mask

class Wav2VecClassifier(nn.Module):
    """wav2vecç‰¹å¾´é‡ç”¨Transformer Encoderåˆ†é¡å™¨ï¼ˆ2å€¤åˆ†é¡ç‰ˆï¼‰"""
    
    def __init__(self):
        super().__init__()
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        model_config = classification_config['model']
        
        self.wav2vec_dim = model_config['wav2vec_dim'] 
        self.hidden_dim = model_config['hidden_size']  
        self.num_classes = model_config['num_classes'] 
        
        # ç‰¹å¾´é‡æŠ•å½±å±¤ï¼ˆwav2vecç‰¹å¾´é‡ã‚’Transformerã®å…¥åŠ›æ¬¡å…ƒã«æŠ•å½±ï¼‰
        self.feature_projection = nn.Linear(self.wav2vec_dim, self.hidden_dim)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.hidden_dim,
            nhead=model_config['n_heads'],
            dim_feedforward=model_config['intermediate_size'], 
            dropout=model_config['dropout'],
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=model_config['n_layers']
        )
        
        # ãƒ—ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        self.pooling = model_config['pooling']
        
        # åˆ†é¡å™¨ï¼ˆ2å€¤åˆ†é¡ç”¨ã€å‡ºåŠ›æ¬¡å…ƒ1ï¼‰
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.hidden_dim),
            nn.Dropout(model_config['dropout']),
            nn.Linear(self.hidden_dim, model_config['hidden_mlp_size']),
            nn.ReLU(),
            nn.Linear(model_config['hidden_mlp_size'], 1)  # å‡ºåŠ›æ¬¡å…ƒ1
        )
    
    def forward(self, features, mask=None):
        """
        wav2vecç‰¹å¾´é‡ç”¨Transformeråˆ†é¡å™¨ã®é †ä¼æ’­

        Args:
            features (torch.Tensor): wav2vecç‰¹å¾´é‡ (batch_size, time_steps, feature_dim)
            mask (torch.BoolTensor, optional): ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
        Returns:
            torch.Tensor: åˆ†é¡å™¨ã®å‡ºåŠ›ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰
        """

        # ç‰¹å¾´é‡æŠ•å½±
        features = self.feature_projection(features)  # (batch_size, time_steps, hidden_dim)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ã®ä½œæˆï¼ˆã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚’ãƒã‚¹ã‚¯ï¼‰
        if mask is None:
            # ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚’æ¤œå‡ºã—ã¦ãƒã‚¹ã‚¯ã‚’ä½œæˆ
            mask = (features.sum(dim=-1) == 0)  # (batch_size, time_steps)
        
        # TransformerEncoderã§ç‰¹å¾´é‡ã‚’å‡¦ç†
        features = self.transformer_encoder(features, src_key_padding_mask=mask)
                
        # ãƒ—ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ã®é©ç”¨
        if self.pooling == 'mean':
            # ãƒã‚¹ã‚¯ã•ã‚ŒãŸéƒ¨åˆ†ã‚’é™¤å¤–ã—ã¦å¹³å‡ã‚’è¨ˆç®—
            if mask is not None:
                features = features.masked_fill(mask.unsqueeze(-1), 0)
                lengths = (~mask).sum(dim=1, keepdim=True).float()
                features = features.sum(dim=1) / lengths.clamp(min=1)
            else:
                features = features.mean(dim=1)  # æ™‚é–“æ¬¡å…ƒã§å¹³å‡
        elif self.pooling == 'cls':
            features = features[:, 0, :]  # æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆCLSãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’ä½¿ç”¨
        
        # æœ€çµ‚åˆ†é¡å™¨ã§ã‚¯ãƒ©ã‚¹ãƒ­ã‚¸ãƒƒãƒˆã‚’å‡ºåŠ›
        return self.classifier(features)

def train_model(model, train_loader, val_loader, num_epochs=None, lr=None):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹é–¢æ•°ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰"""
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    if num_epochs is None:
        num_epochs = classification_config['train']['num_epochs']
    if lr is None:
        lr = classification_config['train']['learning_rate']
    
    # wandbãƒ­ã‚°é »åº¦è¨­å®šã‚’å–å¾—
    wandb_config = config.get('wandb', {})
    logging_config = wandb_config.get('logging', {})
    train_log_every = logging_config.get('train_log_every', 1)
    val_log_every = logging_config.get('val_log_every', 1)
    console_log_every = logging_config.get('console_log_every', 10)
    
    # 2å€¤åˆ†é¡ç”¨ã®æå¤±é–¢æ•°
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=classification_config['train']['weight_decay'])
    # ã‚ˆã‚Šå®‰å®šãªå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)
    
    best_val_acc = 0.0
    train_losses = []
    val_losses = []
    val_accuracies = []
    
    for epoch in range(num_epochs):
        # è¨“ç·´
        model.train()
        train_loss = 0.0
        train_preds = []
        train_true = []
        
        for features, labels, masks in train_loader:
            features, labels = features.to(device), labels.to(device).float()  # ãƒ©ãƒ™ãƒ«ã‚’floatã«å¤‰æ›
            masks = masks.to(device)
            
            optimizer.zero_grad()
            outputs = model(features, masks)
            loss = criterion(outputs.squeeze(1), labels)  # å‡ºåŠ›ã‚’squeezeã—ã¦ãƒ©ãƒ™ãƒ«ã¨å½¢çŠ¶ã‚’åˆã‚ã›ã‚‹
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºã‚’é˜²ãï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            
            # è¨“ç·´æ™‚ã®äºˆæ¸¬ã‚’è¨˜éŒ²
            with torch.no_grad():
                probs = torch.sigmoid(outputs).squeeze(1)
                predictions = (probs >= 0.5).long()
                train_preds.extend(predictions.cpu().numpy())
                train_true.extend(labels.cpu().numpy())
        
        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        # è¨“ç·´æ™‚ã®ç²¾åº¦ã¨F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        train_acc = accuracy_score(train_true, train_preds)
        _, _, train_f1, _ = precision_recall_fscore_support(train_true, train_preds, average='weighted', zero_division=0)
        
        # æ¤œè¨¼
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_true = []
        
        with torch.no_grad():
            for features, labels, masks in val_loader:
                features, labels = features.to(device), labels.to(device).float()  # ãƒ©ãƒ™ãƒ«ã‚’floatã«å¤‰æ›
                masks = masks.to(device)
                outputs = model(features, masks)
                loss = criterion(outputs.squeeze(1), labels)  # å‡ºåŠ›ã‚’squeezeã—ã¦ãƒ©ãƒ™ãƒ«ã¨å½¢çŠ¶ã‚’åˆã‚ã›ã‚‹
                val_loss += loss.item()
                
                # 2å€¤åˆ†é¡ã®æ¨è«–å‡¦ç†
                probs = torch.sigmoid(outputs).squeeze(1)
                predictions = (probs >= 0.5).long()
                val_preds.extend(predictions.cpu().numpy())
                val_true.extend(labels.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_acc = accuracy_score(val_true, val_preds)
        _, _, val_f1, _ = precision_recall_fscore_support(val_true, val_preds, average='weighted', zero_division=0)
        
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
        
        # ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’æ›´æ–°
        scheduler.step()
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        if val_acc > best_val_acc:
            best_val_acc = val_acc
        
        # wandbã«ãƒ­ã‚°ã‚’è¨˜éŒ²ï¼ˆã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰
        wandb.log({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'train_f1': train_f1,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'val_f1': val_f1,
            'learning_rate': optimizer.param_groups[0]['lr']
        }, step=epoch + 1)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼ˆè¨­å®šã•ã‚ŒãŸé »åº¦ã«åŸºã¥ã„ã¦ï¼‰
        if (epoch + 1) % console_log_every == 0:
            logger.info(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')
    
    return {
        'best_val_acc': best_val_acc,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_accuracies': val_accuracies
    }

def evaluate_model(model, test_loader):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°"""
    
    model.eval()
    all_preds = []
    all_true = []
    
    with torch.no_grad():
        for features, labels, masks in test_loader:
            features, labels = features.to(device), labels.to(device)
            masks = masks.to(device)
            outputs = model(features, masks)
            
            # 2å€¤åˆ†é¡ã®æ¨è«–å‡¦ç†
            probs = torch.sigmoid(outputs).squeeze(1)
            predictions = (probs >= 0.5).long()
            
            all_preds.extend(predictions.cpu().numpy())
            all_true.extend(labels.cpu().numpy())
    
    # è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
    accuracy = accuracy_score(all_true, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(all_true, all_preds, average='weighted', zero_division=0)
    conf_matrix = confusion_matrix(all_true, all_preds)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confusion_matrix': conf_matrix,
        'predictions': all_preds,
        'true_labels': all_true
    }

def cross_validate_noise_type(noise_type, n_splits=None):
    """ç‰¹å®šã®ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ã§ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œï¼ˆè¢«é¨“è€…IDãƒ™ãƒ¼ã‚¹åˆ†å‰²ï¼‰"""
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    if n_splits is None:
        n_splits = classification_config['train']['cross_validation_folds']
    
    logger.info(f"Starting cross-validation for noise type: {noise_type}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    dataset = NoiseAugmentedDataset(features_path, noise_type)
    
    if len(dataset) == 0:
        logger.warning(f"No data found for noise type: {noise_type}")
        return None
    
    # è¢«é¨“è€…IDãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    fold_results = []
    
    # è¢«é¨“è€…IDãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²ã‚’å®Ÿè¡Œ
    for fold, (train_idx, val_idx) in enumerate(sgkf.split(dataset.data, dataset.labels, groups=dataset.subject_ids)):
        logger.info(f"Fold {fold + 1}/{n_splits}")
        
        # è¢«é¨“è€…IDã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        train_subjects = set([dataset.subject_ids[i] for i in train_idx])
        val_subjects = set([dataset.subject_ids[i] for i in val_idx])
        overlap = train_subjects.intersection(val_subjects)
        
        if overlap:
            logger.warning(f"Fold {fold + 1}: Found overlapping subjects between train and validation: {overlap}")
        else:
            logger.info(f"Fold {fold + 1}: No overlapping subjects between train and validation")
        
        logger.info(f"Fold {fold + 1}: Train subjects: {len(train_subjects)}, Validation subjects: {len(val_subjects)}")
        logger.info(f"Fold {fold + 1}: Train samples: {len(train_idx)}, Validation samples: {len(val_idx)}")
        
        # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã”ã¨ã«æ–°ã—ã„wandb runã‚’ä½œæˆ
        wandb.init(
            project="noise-augmented-wav2vec-classification",
            name=f"{noise_type}_fold_{fold + 1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            config={
                "noise_type": noise_type,
                "fold": fold + 1,
                "model_name": classification_config['model_name'],
                "batch_size": classification_config['train']['batch_size'],
                "learning_rate": classification_config['train']['learning_rate'],
                "num_epochs": classification_config['train']['num_epochs'],
                "hidden_size": classification_config['model']['hidden_size'],
                "n_layers": classification_config['model']['n_layers'],
                "n_heads": classification_config['model']['n_heads']
            }
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆï¼ˆå†ç¾æ€§ã®ãŸã‚worker_init_fnã‚’è¨­å®šï¼‰
        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)
        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)
        
        def worker_init_fn(worker_id):
            """DataLoaderã®ãƒ¯ãƒ¼ã‚«ãƒ¼åˆæœŸåŒ–é–¢æ•°ï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰"""
            np.random.seed(42 + worker_id)
        
        train_loader = DataLoader(
            dataset, 
            batch_size=classification_config['train']['batch_size'], 
            sampler=train_sampler,
            worker_init_fn=worker_init_fn,
            num_workers=0  # å†ç¾æ€§ã®ãŸã‚0ã«è¨­å®š
        )
        val_loader = DataLoader(
            dataset, 
            batch_size=classification_config['train']['batch_size'], 
            sampler=val_sampler,
            worker_init_fn=worker_init_fn,
            num_workers=0  # å†ç¾æ€§ã®ãŸã‚0ã«è¨­å®š
        )
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—ï¼‰
        model = Wav2VecClassifier().to(device)
        
        # è¨“ç·´
        train_history = train_model(model, train_loader, val_loader)
        
        # è©•ä¾¡
        eval_results = evaluate_model(model, val_loader)
        
        fold_results.append({
            'fold': fold + 1,
            'best_val_acc': train_history['best_val_acc'],
            'final_val_acc': eval_results['accuracy'],
            'precision': eval_results['precision'],
            'recall': eval_results['recall'],
            'f1_score': eval_results['f1_score'],
            'train_history': train_history
        })
        
        # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®æœ€çµ‚çµæœã‚’wandbã«è¨˜éŒ²
        wandb.log({
            'final_accuracy': eval_results['accuracy'],
            'final_precision': eval_results['precision'],
            'final_recall': eval_results['recall'],
            'final_f1_score': eval_results['f1_score']
        })
        
        # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®wandb runã‚’çµ‚äº†
        wandb.finish()
    
    # å¹³å‡çµæœã‚’è¨ˆç®—
    mean_accuracy = np.mean([r['final_val_acc'] for r in fold_results])
    std_accuracy = np.std([r['final_val_acc'] for r in fold_results])
    mean_f1 = np.mean([r['f1_score'] for r in fold_results])
    std_f1 = np.std([r['f1_score'] for r in fold_results])
    
    avg_results = {
        'noise_type': noise_type,
        'mean_accuracy': mean_accuracy,
        'std_accuracy': std_accuracy,
        'mean_f1': mean_f1,
        'std_f1': std_f1,
        'fold_results': fold_results
    }
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã®çµæœã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã®æ–°ã—ã„wandb run
    wandb.init(
        project="noise-augmented-wav2vec-classification",
        name=f"{noise_type}_cv_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        config={
            "noise_type": noise_type,
            "cv_folds": n_splits,
            "model_name": classification_config['model_name']
        }
    )
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã®çµæœã‚’è¨˜éŒ²
    wandb.log({
        'cv_mean_accuracy': mean_accuracy,
        'cv_std_accuracy': std_accuracy,
        'cv_mean_f1': mean_f1,
        'cv_std_f1': std_f1
    })
    
    # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®è©³ç´°çµæœã‚‚è¨˜éŒ²
    for i, result in enumerate(fold_results):
        wandb.log({
            f'fold_{i+1}_accuracy': result['final_val_acc'],
            f'fold_{i+1}_f1': result['f1_score']
        })
    
    wandb.finish()
    
    return avg_results


def process_single_noise_type(noise_type):
    """å˜ä¸€ã®ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ã‚’å‡¦ç†"""
    logger.info(f"Processing single noise type: {noise_type}")
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    results = cross_validate_noise_type(noise_type)
    
    if results is None:
        logger.error(f"Failed to process noise type: {noise_type}")
        return None
    
    # çµæœã‚’ä¿å­˜
    noise_output_path = os.path.join(output_path, noise_type)
    os.makedirs(noise_output_path, exist_ok=True)
    
    # CSVã§ä¿å­˜
    results_df = pd.DataFrame([{
        'noise_type': results['noise_type'],
        'mean_accuracy': results['mean_accuracy'],
        'std_accuracy': results['std_accuracy'],
        'mean_f1': results['mean_f1'],
        'std_f1': results['std_f1']
    }])
    
    results_df.to_csv(os.path.join(noise_output_path, f'{noise_type}_results.csv'), index=False)
    
    # è©³ç´°çµæœã‚’JSONã§ä¿å­˜
    with open(os.path.join(noise_output_path, f'{noise_type}_detailed_results.json'), 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    # çµæœã‚’è¡¨ç¤º
    logger.info(f"Results for {noise_type}:")
    logger.info(f"  Mean Accuracy: {results['mean_accuracy']:.4f} Â± {results['std_accuracy']:.4f}")
    logger.info(f"  Mean F1: {results['mean_f1']:.4f} Â± {results['std_f1']:.4f}")
    
    return results


if __name__ == "__main__":
    logger.info("Starting noise augmented classification comparison with subject-based splitting...")
    
    # ã™ã¹ã¦ã®ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ã‚’å‡¦ç†
    logger.info("Processing all available noise types with subject-based cross-validation")
    logger.info("Using StratifiedGroupKFold to ensure no subject data appears in both train and test sets")
    available_noise_types = ["original", "gaussian_noise_light", "gaussian_noise_medium", "gaussian_noise_heavy", "uniform_noise"]
    
    logger.info(f"Found {len(available_noise_types)} noise types: {available_noise_types}")
    
    all_results = {}
    for noise_type in available_noise_types:
        logger.info(f"\n{'='*50}")
        logger.info(f"Processing noise type: {noise_type}")
        logger.info(f"{'='*50}")
        
        try:
            result = process_single_noise_type(noise_type)
            if result:
                all_results[noise_type] = result
                logger.info(f"âœ“ Completed processing for {noise_type}")
                logger.info(f"  Mean Accuracy: {result['mean_accuracy']:.4f} Â± {result['std_accuracy']:.4f}")
                logger.info(f"  Mean F1: {result['mean_f1']:.4f} Â± {result['std_f1']:.4f}")
            else:
                logger.error(f"âœ— Failed to process noise type: {noise_type}")
        except Exception as e:
            logger.error(f"âœ— Error processing noise type {noise_type}: {str(e)}")
            continue
    
    # å…¨çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    if all_results:
        logger.info(f"\n{'='*50}")
        logger.info("SUMMARY OF ALL NOISE TYPES")
        logger.info(f"{'='*50}")
        
        # çµæœã‚’ç²¾åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(all_results.items(), key=lambda x: x[1]['mean_accuracy'], reverse=True)
        
        for i, (noise_type, result) in enumerate(sorted_results, 1):
            logger.info(f"{i}. {noise_type}:")
            logger.info(f"   Accuracy: {result['mean_accuracy']:.4f} Â± {result['std_accuracy']:.4f}")
            logger.info(f"   F1 Score: {result['mean_f1']:.4f} Â± {result['std_f1']:.4f}")
        
        # æœ€è‰¯ã®çµæœã‚’å¼·èª¿
        best_noise_type, best_result = sorted_results[0]
        logger.info(f"\nğŸ† BEST PERFORMANCE: {best_noise_type}")
        logger.info(f"   Accuracy: {best_result['mean_accuracy']:.4f} Â± {best_result['std_accuracy']:.4f}")
        logger.info(f"   F1 Score: {best_result['mean_f1']:.4f} Â± {best_result['std_f1']:.4f}")
        
        # çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        summary_df = pd.DataFrame([
            {
                'noise_type': noise_type,
                'mean_accuracy': result['mean_accuracy'],
                'std_accuracy': result['std_accuracy'],
                'mean_f1': result['mean_f1'],
                'std_f1': result['std_f1']
            }
            for noise_type, result in sorted_results
        ])
        
        summary_path = os.path.join(output_path, 'all_noise_types_summary.csv')
        summary_df.to_csv(summary_path, index=False)
        logger.info(f"\nSummary saved to: {summary_path}")
        
    else:
        logger.warning("No results to summarize")
    
    logger.info("Noise augmented classification comparison with subject-based splitting completed!")