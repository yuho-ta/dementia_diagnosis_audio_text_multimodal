(venv) PS C:\Users\yuho2\CogniAlign\modules> python silence_only_transformer.py --data_type original
wandb: Currently logged in as: yuho2074tamura1217 (yuho2074tamura1217-keio-university-global-page) to
 https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in C:\Users\yuho2\CogniAlign\modules\wandb\run-20250903_190949-v1nqv5z6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-elevator-12
wandb:  View project at https://wandb.ai/yuho2074tamura1217-keio-university-global-page/silence-transformer-classification
wandb:  View run at https://wandb.ai/yuho2074tamura1217-keio-university-global-page/silence-transformer-classification/runs/v1nqv5z6
Loading silence features with ID-based split...
Total unique IDs found: 282
Train IDs: 225, Test IDs: 57
Original train samples loaded: 225
Noise augmented train samples loaded: 225
Test samples loaded: 57
Original train data: 225 samples (CN: 75, AD: 150)
Noise augmented train data: 225 samples (CN: 75, AD: 150)
Test data: 57 samples (CN: 17, AD: 40)

==================================================
Training model with original features
==================================================

### Fold 1 - ORIGINAL Features
Fold 1 train label distribution:
  CN (0): 60 samples
  AD (1): 120 samples
  Total: 180 samples
Fold 1 validation label distribution:
  CN (0): 15 samples
  AD (1): 30 samples
  Total: 45 samples
Overall train dataset distribution:
  CN (0): 75 samples
  AD (1): 150 samples
  Total: 225 samples
Class distribution: Class 0 (CN): 60, Class 1 (AD): 120
Calculated weights: Class 0: 1.5000, Class 1: 0.7500
pos_weight for BCEWithLogitsLoss: 0.5000
Fold 1 pos_weight: 0.5000
Epoch 1/5: 100%|███████████████████████████████████████| 23/23 [03:37<00:00,  9.47s/it, loss=0.3334]
Epoch 1: Train Loss: 0.4579, Val Loss: 0.4531, Val Accuracy: 0.6667
  Precision: 0.6667, Recall: 1.0000, F1: 0.8000, AUC: 0.5711
Epoch 2/5: 100%|███████████████████████████████████████| 23/23 [03:43<00:00,  9.72s/it, loss=0.4228] 
Epoch 2: Train Loss: 0.4585, Val Loss: 0.4524, Val Accuracy: 0.6667
  Precision: 0.6667, Recall: 1.0000, F1: 0.8000, AUC: 0.5756
Epoch 3/5: 100%|███████████████████████████████████████| 23/23 [03:34<00:00,  9.33s/it, loss=0.5142]
Epoch 3: Train Loss: 0.4585, Val Loss: 0.4521, Val Accuracy: 0.6889
  Precision: 0.6818, Recall: 1.0000, F1: 0.8108, AUC: 0.5800
Epoch 4/5: 100%|███████████████████████████████████████| 23/23 [03:40<00:00,  9.59s/it, loss=0.6198] 
Epoch 4: Train Loss: 0.4598, Val Loss: 0.4518, Val Accuracy: 0.6667
  Precision: 0.6829, Recall: 0.9333, F1: 0.7887, AUC: 0.5844
Epoch 5/5: 100%|███████████████████████████████████████| 23/23 [03:41<00:00,  9.63s/it, loss=0.4318] 
Epoch 5: Train Loss: 0.4547, Val Loss: 0.4515, Val Accuracy: 0.6667
  Precision: 0.6744, Recall: 0.9667, F1: 0.7945, AUC: 0.5911
Best model state restored for fold 1 testing
Fold 1 (original) completed. Best F1-score: 0.8108, Best Accuracy: 0.6889

### Fold 2 - ORIGINAL Features
Fold 2 train label distribution:
  CN (0): 60 samples
  AD (1): 120 samples
  Total: 180 samples
Fold 2 validation label distribution:
  CN (0): 15 samples
  AD (1): 30 samples
  Total: 45 samples
Overall train dataset distribution:
  CN (0): 75 samples
  AD (1): 150 samples
  Total: 225 samples
Class distribution: Class 0 (CN): 60, Class 1 (AD): 120
Calculated weights: Class 0: 1.5000, Class 1: 0.7500
pos_weight for BCEWithLogitsLoss: 0.5000
Fold 2 pos_weight: 0.5000
Epoch 1/5: 100%|███████████████████████████████████████| 23/23 [04:07<00:00, 10.78s/it, loss=0.5360]
Epoch 1: Train Loss: 0.4650, Val Loss: 0.4526, Val Accuracy: 0.6667
  Precision: 0.6667, Recall: 1.0000, F1: 0.8000, AUC: 0.7689
Epoch 2/5: 100%|███████████████████████████████████████| 23/23 [03:41<00:00,  9.61s/it, loss=0.4220]
Epoch 2: Train Loss: 0.4609, Val Loss: 0.4515, Val Accuracy: 0.6667
  Precision: 0.6667, Recall: 1.0000, F1: 0.8000, AUC: 0.7644
Epoch 3/5: 100%|███████████████████████████████████████| 23/23 [03:39<00:00,  9.56s/it, loss=0.6311]
Epoch 3: Train Loss: 0.4635, Val Loss: 0.4503, Val Accuracy: 0.6667
  Precision: 0.6667, Recall: 1.0000, F1: 0.8000, AUC: 0.7489
Epoch 4/5: 100%|███████████████████████████████████████| 23/23 [03:33<00:00,  9.30s/it, loss=0.6178] 
Epoch 4: Train Loss: 0.4619, Val Loss: 0.4494, Val Accuracy: 0.6667
  Precision: 0.6667, Recall: 1.0000, F1: 0.8000, AUC: 0.7444
Epoch 5/5: 100%|███████████████████████████████████████| 23/23 [03:42<00:00,  9.68s/it, loss=0.3275] 
Epoch 5: Train Loss: 0.4566, Val Loss: 0.4490, Val Accuracy: 0.6667
  Precision: 0.6667, Recall: 1.0000, F1: 0.8000, AUC: 0.7444
Best model state restored for fold 2 testing
Fold 2 (original) completed. Best F1-score: 0.8000, Best Accuracy: 0.6667

### Fold 3 - ORIGINAL Features
Fold 3 train label distribution:
  CN (0): 60 samples
  AD (1): 120 samples
  Total: 180 samples
Fold 3 validation label distribution:
  CN (0): 15 samples
  AD (1): 30 samples
  Total: 45 samples
Overall train dataset distribution:
  CN (0): 75 samples
  AD (1): 150 samples
  Total: 225 samples
Class distribution: Class 0 (CN): 60, Class 1 (AD): 120
Calculated weights: Class 0: 1.5000, Class 1: 0.7500
pos_weight for BCEWithLogitsLoss: 0.5000
Fold 3 pos_weight: 0.5000
Epoch 1/5: 100%|███████████████████████████████████████| 23/23 [03:36<00:00,  9.40s/it, loss=0.3394]
Epoch 1: Train Loss: 0.4594, Val Loss: 0.4528, Val Accuracy: 0.6667
  Precision: 0.6667, Recall: 1.0000, F1: 0.8000, AUC: 0.7044
Epoch 2/5: 100%|███████████████████████████████████████| 23/23 [03:32<00:00,  9.23s/it, loss=0.4283]
Epoch 2: Train Loss: 0.4597, Val Loss: 0.4517, Val Accuracy: 0.6222
  Precision: 0.8095, Recall: 0.5667, F1: 0.6667, AUC: 0.7244
Epoch 3/5: 100%|███████████████████████████████████████| 23/23 [03:44<00:00,  9.75s/it, loss=0.5020] 
Epoch 3: Train Loss: 0.4616, Val Loss: 0.4518, Val Accuracy: 0.4444
  Precision: 0.8571, Recall: 0.2000, F1: 0.3243, AUC: 0.7000
Epoch 4/5: 100%|███████████████████████████████████████| 23/23 [03:30<00:00,  9.17s/it, loss=0.4274]
Epoch 4: Train Loss: 0.4586, Val Loss: 0.4513, Val Accuracy: 0.4222
  Precision: 0.8333, Recall: 0.1667, F1: 0.2778, AUC: 0.7089
Epoch 5/5: 100%|███████████████████████████████████████| 23/23 [03:32<00:00,  9.25s/it, loss=0.5187] 
Epoch 5: Train Loss: 0.4587, Val Loss: 0.4506, Val Accuracy: 0.4444
  Precision: 0.8571, Recall: 0.2000, F1: 0.3243, AUC: 0.7156
Best model state restored for fold 3 testing
Fold 3 (original) completed. Best F1-score: 0.8000, Best Accuracy: 0.6667

### Fold 4 - ORIGINAL Features
Fold 4 train label distribution:
  CN (0): 60 samples
  AD (1): 120 samples
  Total: 180 samples
Fold 4 validation label distribution:
  CN (0): 15 samples
  AD (1): 30 samples
  Total: 45 samples
Overall train dataset distribution:
  CN (0): 75 samples
  AD (1): 150 samples
  Total: 225 samples
Class distribution: Class 0 (CN): 60, Class 1 (AD): 120
Calculated weights: Class 0: 1.5000, Class 1: 0.7500
pos_weight for BCEWithLogitsLoss: 0.5000
Fold 4 pos_weight: 0.5000
Epoch 1/5: 100%|███████████████████████████████████████| 23/23 [03:48<00:00,  9.94s/it, loss=0.4338]
Epoch 1: Train Loss: 0.4646, Val Loss: 0.4548, Val Accuracy: 0.6444
  Precision: 0.6591, Recall: 0.9667, F1: 0.7838, AUC: 0.4422
Epoch 2/5: 100%|███████████████████████████████████████| 23/23 [03:38<00:00,  9.52s/it, loss=0.3463] 
Epoch 2: Train Loss: 0.4599, Val Loss: 0.4532, Val Accuracy: 0.6222
  Precision: 0.8095, Recall: 0.5667, F1: 0.6667, AUC: 0.6822
Epoch 3/5: 100%|███████████████████████████████████████| 23/23 [03:40<00:00,  9.61s/it, loss=0.4254] 
Epoch 3: Train Loss: 0.4608, Val Loss: 0.4530, Val Accuracy: 0.3778
  Precision: 1.0000, Recall: 0.0667, F1: 0.1250, AUC: 0.6978
Epoch 4/5: 100%|███████████████████████████████████████| 23/23 [03:38<00:00,  9.50s/it, loss=0.5043]
Epoch 4: Train Loss: 0.4608, Val Loss: 0.4519, Val Accuracy: 0.4222
  Precision: 1.0000, Recall: 0.1333, F1: 0.2353, AUC: 0.7111
Epoch 5/5: 100%|███████████████████████████████████████| 23/23 [03:40<00:00,  9.59s/it, loss=0.5117] 
Epoch 5: Train Loss: 0.4601, Val Loss: 0.4512, Val Accuracy: 0.4667
  Precision: 1.0000, Recall: 0.2000, F1: 0.3333, AUC: 0.7133
Best model state restored for fold 4 testing
Fold 4 (original) completed. Best F1-score: 0.7838, Best Accuracy: 0.6444

### Fold 5 - ORIGINAL Features
Fold 5 train label distribution:
  CN (0): 60 samples
  AD (1): 120 samples
  Total: 180 samples
Fold 5 validation label distribution:
  CN (0): 15 samples
  AD (1): 30 samples
  Total: 45 samples
Overall train dataset distribution:
  CN (0): 75 samples
  AD (1): 150 samples
  Total: 225 samples
Class distribution: Class 0 (CN): 60, Class 1 (AD): 120
Calculated weights: Class 0: 1.5000, Class 1: 0.7500
pos_weight for BCEWithLogitsLoss: 0.5000
Fold 5 pos_weight: 0.5000
Epoch 1/5: 100%|███████████████████████████████████████| 23/23 [03:37<00:00,  9.46s/it, loss=0.4381]
Epoch 1: Train Loss: 0.4607, Val Loss: 0.4549, Val Accuracy: 0.4000
  Precision: 1.0000, Recall: 0.1000, F1: 0.1818, AUC: 0.5244
Epoch 2/5: 100%|███████████████████████████████████████| 23/23 [03:39<00:00,  9.55s/it, loss=0.4943] 
Epoch 2: Train Loss: 0.4602, Val Loss: 0.4532, Val Accuracy: 0.4667
  Precision: 0.8000, Recall: 0.2667, F1: 0.4000, AUC: 0.5844
Epoch 3/5: 100%|███████████████████████████████████████| 23/23 [03:40<00:00,  9.60s/it, loss=0.4413] 
Epoch 3: Train Loss: 0.4563, Val Loss: 0.4510, Val Accuracy: 0.6222
  Precision: 0.7600, Recall: 0.6333, F1: 0.6909, AUC: 0.5978
Epoch 4/5: 100%|███████████████████████████████████████| 23/23 [03:34<00:00,  9.35s/it, loss=0.4271] 
Epoch 4: Train Loss: 0.4544, Val Loss: 0.4500, Val Accuracy: 0.6444
  Precision: 0.7500, Recall: 0.7000, F1: 0.7241, AUC: 0.6133
Epoch 5/5: 100%|███████████████████████████████████████| 23/23 [03:44<00:00,  9.75s/it, loss=0.6111]
Epoch 5: Train Loss: 0.4564, Val Loss: 0.4496, Val Accuracy: 0.6444
  Precision: 0.7500, Recall: 0.7000, F1: 0.7241, AUC: 0.6133
Best model state restored for fold 5 testing
Fold 5 (original) completed. Best F1-score: 0.7241, Best Accuracy: 0.6444

=== AD Sample Usage Across All Folds (ORIGINAL) ===
Fold 1: 150 AD samples
Fold 2: 150 AD samples
Fold 3: 150 AD samples
Fold 4: 150 AD samples
Fold 5: 150 AD samples
Total unique AD samples used across all folds: 150
Total AD samples in train dataset: 150
All AD samples used: Yes

=== Final Results (ORIGINAL) ===
   fold data_type   best_f1  best_val_accuracy  best_val_loss  final_val_accuracy  final_val_loss
0     1  original  0.810811           0.688889       0.451473            0.666667        0.451473    
1     2  original  0.800000           0.666667       0.448953            0.666667        0.448953    
2     3  original  0.800000           0.666667       0.450595            0.444444        0.450595    
3     4  original  0.783784           0.644444       0.451250            0.466667        0.451250    
4     5  original  0.724138           0.644444       0.449582            0.644444        0.449582    

=== Test Performance Across Folds (ORIGINAL) (Average ± Std) ===
Test F1-score: 0.6553 ± 0.1981
Test Accuracy: 0.6105 ± 0.1267
Test AUC: 0.7300 ± 0.0197

=== Best Values Across Epochs (ORIGINAL) (Average ± Std) ===
Best F1-score: 0.7837 ± 0.0347
Best Accuracy: 0.6622 ± 0.0186
Best Loss: 0.4504 ± 0.0011

=== Test Data Evaluation (ORIGINAL) ===
Test data contains 57 samples
Test CN samples: 17, Test AD samples: 40

ORIGINAL model training completed!

============================================================
FINAL PERFORMANCE SUMMARY - ORIGINAL MODEL
============================================================
Test F1-Score: 0.6553 ± 0.1981
Test Accuracy: 0.6105 ± 0.1267
Test AUC: 0.7300 ± 0.0197
Cross-validation folds: 5
Training samples: 225
Test samples: 57
wandb:
wandb:
wandb: Run history:
wandb:  dataset/noise_augmented_train_ad_samples ▁
wandb:  dataset/noise_augmented_train_cn_samples ▁
wandb:     dataset/noise_augmented_train_samples ▁
wandb:         dataset/original_train_ad_samples ▁
wandb:         dataset/original_train_cn_samples ▁
wandb:            dataset/original_train_samples ▁
wandb:                   dataset/test_ad_samples ▁
wandb:                   dataset/test_cn_samples ▁
wandb:                      dataset/test_samples ▁
wandb:                     dataset/total_samples ▁
wandb:                    final_summary/cv_folds ▁
wandb:               final_summary/test_accuracy ▁
wandb:           final_summary/test_accuracy_std ▁
wandb:                    final_summary/test_auc ▁
wandb:                final_summary/test_auc_std ▁
wandb:               final_summary/test_f1_score ▁
wandb:           final_summary/test_f1_score_std ▁
wandb:                final_summary/test_samples ▁
wandb:            final_summary/training_samples ▁
wandb:         fold_1_original_transformer/epoch ▁▃▅▆█
wandb: fold_1_original_transformer/learning_rate █▆▄▃▁
wandb:    fold_1_original_transformer/train_loss ▅▆▆█▁
wandb:  fold_1_original_transformer/val_accuracy ▁▁█▁▁
wandb:   fold_1_original_transformer/val_auc_roc ▁▃▄▆█
wandb:        fold_1_original_transformer/val_f1 ▅▅█▁▃
wandb:      fold_1_original_transformer/val_loss █▅▄▂▁
wandb: fold_1_original_transformer/val_precision ▁▁██▄
wandb:    fold_1_original_transformer/val_recall ███▁▄
wandb:         fold_2_original_transformer/epoch ▁▃▅▆█
wandb: fold_2_original_transformer/learning_rate █▆▄▃▁
wandb:    fold_2_original_transformer/train_loss █▅▇▅▁
wandb:  fold_2_original_transformer/val_accuracy ▁▁▁▁▁
wandb:   fold_2_original_transformer/val_auc_roc █▇▂▁▁
wandb:        fold_2_original_transformer/val_f1 ▁▁▁▁▁
wandb:      fold_2_original_transformer/val_loss █▆▄▂▁
wandb: fold_2_original_transformer/val_precision ▁▁▁▁▁
wandb:    fold_2_original_transformer/val_recall ▁▁▁▁▁
wandb:         fold_3_original_transformer/epoch ▁▃▅▆█
wandb: fold_3_original_transformer/learning_rate █▆▄▃▁
wandb:    fold_3_original_transformer/train_loss ▃▄█▁▁
wandb:  fold_3_original_transformer/val_accuracy █▇▂▁▂
wandb:   fold_3_original_transformer/val_auc_roc ▂█▁▄▅
wandb:        fold_3_original_transformer/val_f1 █▆▂▁▂
wandb:      fold_3_original_transformer/val_loss █▄▅▃▁
wandb: fold_3_original_transformer/val_precision ▁▆█▇█
wandb:    fold_3_original_transformer/val_recall █▄▁▁▁
wandb:         fold_4_original_transformer/epoch ▁▃▅▆█
wandb: fold_4_original_transformer/learning_rate █▆▄▃▁
wandb:    fold_4_original_transformer/train_loss █▁▂▂▁
wandb:  fold_4_original_transformer/val_accuracy █▇▁▂▃
wandb:   fold_4_original_transformer/val_auc_roc ▁▇███
wandb:        fold_4_original_transformer/val_f1 █▇▁▂▃
wandb:      fold_4_original_transformer/val_loss █▅▅▂▁
wandb: fold_4_original_transformer/val_precision ▁▄███
wandb:    fold_4_original_transformer/val_recall █▅▁▂▂
wandb:         fold_5_original_transformer/epoch ▁▃▅▆█
wandb: fold_5_original_transformer/learning_rate █▆▄▃▁
wandb:    fold_5_original_transformer/train_loss █▇▃▁▃
wandb:  fold_5_original_transformer/val_accuracy ▁▃▇██
wandb:   fold_5_original_transformer/val_auc_roc ▁▆▇██
wandb:        fold_5_original_transformer/val_f1 ▁▄███
wandb:      fold_5_original_transformer/val_loss █▆▃▂▁
wandb: fold_5_original_transformer/val_precision █▂▁▁▁
wandb:    fold_5_original_transformer/val_recall ▁▃▇██
wandb:         original_final/mean_best_accuracy ▁
wandb:               original_final/mean_best_f1 ▁
wandb:             original_final/mean_best_loss ▁
wandb:          original_final/std_best_accuracy ▁
wandb:                original_final/std_best_f1 ▁
wandb:              original_final/std_best_loss ▁
wandb:                   original_fold_1/best_f1 ▁
wandb:         original_fold_1/best_val_accuracy ▁
wandb:             original_fold_1/best_val_loss ▁
wandb:        original_fold_1/final_val_accuracy ▁
wandb:            original_fold_1/final_val_loss ▁
wandb:                original_fold_1/pos_weight ▁
wandb:             original_fold_1/test_accuracy ▁
wandb:                  original_fold_1/test_auc ▁
wandb:                   original_fold_1/test_f1 ▁
wandb:                 original_fold_1/test_loss ▁
wandb:                  original_fold_1/train_ad ▁
wandb:            original_fold_1/train_ad_ratio ▁
wandb:                  original_fold_1/train_cn ▁
wandb:            original_fold_1/train_cn_ratio ▁
wandb:               original_fold_1/train_total ▁
wandb:                    original_fold_1/val_ad ▁
wandb:              original_fold_1/val_ad_ratio ▁
wandb:                    original_fold_1/val_cn ▁
wandb:              original_fold_1/val_cn_ratio ▁
wandb:                 original_fold_1/val_total ▁
wandb:                   original_fold_2/best_f1 ▁
wandb:         original_fold_2/best_val_accuracy ▁
wandb:             original_fold_2/best_val_loss ▁
wandb:        original_fold_2/final_val_accuracy ▁
wandb:            original_fold_2/final_val_loss ▁
wandb:                original_fold_2/pos_weight ▁
wandb:             original_fold_2/test_accuracy ▁
wandb:                  original_fold_2/test_auc ▁
wandb:                   original_fold_2/test_f1 ▁
wandb:                 original_fold_2/test_loss ▁
wandb:                  original_fold_2/train_ad ▁
wandb:            original_fold_2/train_ad_ratio ▁
wandb:                  original_fold_2/train_cn ▁
wandb:            original_fold_2/train_cn_ratio ▁
wandb:               original_fold_2/train_total ▁
wandb:                    original_fold_2/val_ad ▁
wandb:              original_fold_2/val_ad_ratio ▁
wandb:                    original_fold_2/val_cn ▁
wandb:              original_fold_2/val_cn_ratio ▁
wandb:                 original_fold_2/val_total ▁
wandb:                   original_fold_3/best_f1 ▁
wandb:         original_fold_3/best_val_accuracy ▁
wandb:             original_fold_3/best_val_loss ▁
wandb:        original_fold_3/final_val_accuracy ▁
wandb:            original_fold_3/final_val_loss ▁
wandb:                original_fold_3/pos_weight ▁
wandb:             original_fold_3/test_accuracy ▁
wandb:                  original_fold_3/test_auc ▁
wandb:                   original_fold_3/test_f1 ▁
wandb:                 original_fold_3/test_loss ▁
wandb:                  original_fold_3/train_ad ▁
wandb:            original_fold_3/train_ad_ratio ▁
wandb:                  original_fold_3/train_cn ▁
wandb:            original_fold_3/train_cn_ratio ▁
wandb:               original_fold_3/train_total ▁
wandb:                    original_fold_3/val_ad ▁
wandb:              original_fold_3/val_ad_ratio ▁
wandb:                    original_fold_3/val_cn ▁
wandb:              original_fold_3/val_cn_ratio ▁
wandb:                 original_fold_3/val_total ▁
wandb:                   original_fold_4/best_f1 ▁
wandb:         original_fold_4/best_val_accuracy ▁
wandb:             original_fold_4/best_val_loss ▁
wandb:        original_fold_4/final_val_accuracy ▁
wandb:            original_fold_4/final_val_loss ▁
wandb:                original_fold_4/pos_weight ▁
wandb:             original_fold_4/test_accuracy ▁
wandb:                  original_fold_4/test_auc ▁
wandb:                   original_fold_4/test_f1 ▁
wandb:                 original_fold_4/test_loss ▁
wandb:                  original_fold_4/train_ad ▁
wandb:            original_fold_4/train_ad_ratio ▁
wandb:                  original_fold_4/train_cn ▁
wandb:            original_fold_4/train_cn_ratio ▁
wandb:               original_fold_4/train_total ▁
wandb:                    original_fold_4/val_ad ▁
wandb:              original_fold_4/val_ad_ratio ▁
wandb:                    original_fold_4/val_cn ▁
wandb:              original_fold_4/val_cn_ratio ▁
wandb:                 original_fold_4/val_total ▁
wandb:                   original_fold_5/best_f1 ▁
wandb:         original_fold_5/best_val_accuracy ▁
wandb:             original_fold_5/best_val_loss ▁
wandb:        original_fold_5/final_val_accuracy ▁
wandb:            original_fold_5/final_val_loss ▁
wandb:                original_fold_5/pos_weight ▁
wandb:             original_fold_5/test_accuracy ▁
wandb:                  original_fold_5/test_auc ▁
wandb:                   original_fold_5/test_f1 ▁
wandb:                 original_fold_5/test_loss ▁
wandb:                  original_fold_5/train_ad ▁
wandb:            original_fold_5/train_ad_ratio ▁
wandb:                  original_fold_5/train_cn ▁
wandb:            original_fold_5/train_cn_ratio ▁
wandb:               original_fold_5/train_total ▁
wandb:                    original_fold_5/val_ad ▁
wandb:              original_fold_5/val_ad_ratio ▁
wandb:                    original_fold_5/val_cn ▁
wandb:              original_fold_5/val_cn_ratio ▁
wandb:                 original_fold_5/val_total ▁
wandb:               original_test/mean_accuracy ▁
wandb:                    original_test/mean_auc ▁
wandb:                     original_test/mean_f1 ▁
wandb:                original_test/std_accuracy ▁
wandb:                     original_test/std_auc ▁
wandb:                      original_test/std_f1 ▁
wandb:
wandb: Run summary:
wandb:  dataset/noise_augmented_train_ad_samples 150
wandb:  dataset/noise_augmented_train_cn_samples 75
wandb:     dataset/noise_augmented_train_samples 225
wandb:         dataset/original_train_ad_samples 150
wandb:         dataset/original_train_cn_samples 75
wandb:            dataset/original_train_samples 225
wandb:                   dataset/test_ad_samples 40
wandb:                   dataset/test_cn_samples 17
wandb:                      dataset/test_samples 57
wandb:                     dataset/total_samples 507
wandb:                    final_summary/cv_folds 5
wandb:                   final_summary/data_type original
wandb:               final_summary/test_accuracy 0.61053
wandb:           final_summary/test_accuracy_std 0.12671
wandb:                    final_summary/test_auc 0.73
wandb:                final_summary/test_auc_std 0.01972
wandb:               final_summary/test_f1_score 0.65532
wandb:           final_summary/test_f1_score_std 0.19806
wandb:                final_summary/test_samples 57
wandb:            final_summary/training_samples 225
wandb:         fold_1_original_transformer/epoch 5
wandb: fold_1_original_transformer/learning_rate 0.0
wandb:    fold_1_original_transformer/train_loss 0.45467
wandb:  fold_1_original_transformer/val_accuracy 0.66667
wandb:   fold_1_original_transformer/val_auc_roc 0.59111
wandb:        fold_1_original_transformer/val_f1 0.79452
wandb:      fold_1_original_transformer/val_loss 0.45147
wandb: fold_1_original_transformer/val_precision 0.67442
wandb:    fold_1_original_transformer/val_recall 0.96667
wandb:         fold_2_original_transformer/epoch 5
wandb: fold_2_original_transformer/learning_rate 0.0
wandb:    fold_2_original_transformer/train_loss 0.45661
wandb:  fold_2_original_transformer/val_accuracy 0.66667
wandb:   fold_2_original_transformer/val_auc_roc 0.74444
wandb:        fold_2_original_transformer/val_f1 0.8
wandb:      fold_2_original_transformer/val_loss 0.44895
wandb: fold_2_original_transformer/val_precision 0.66667
wandb:    fold_2_original_transformer/val_recall 1
wandb:         fold_3_original_transformer/epoch 5
wandb: fold_3_original_transformer/learning_rate 0.0
wandb:    fold_3_original_transformer/train_loss 0.45874
wandb:  fold_3_original_transformer/val_accuracy 0.44444
wandb:   fold_3_original_transformer/val_auc_roc 0.71556
wandb:        fold_3_original_transformer/val_f1 0.32432
wandb:      fold_3_original_transformer/val_loss 0.45059
wandb: fold_3_original_transformer/val_precision 0.85714
wandb:    fold_3_original_transformer/val_recall 0.2
wandb:         fold_4_original_transformer/epoch 5
wandb: fold_4_original_transformer/learning_rate 0.0
wandb:    fold_4_original_transformer/train_loss 0.46013
wandb:  fold_4_original_transformer/val_accuracy 0.46667
wandb:   fold_4_original_transformer/val_auc_roc 0.71333
wandb:        fold_4_original_transformer/val_f1 0.33333
wandb:      fold_4_original_transformer/val_loss 0.45125
wandb: fold_4_original_transformer/val_precision 1
wandb:    fold_4_original_transformer/val_recall 0.2
wandb:         fold_5_original_transformer/epoch 5
wandb: fold_5_original_transformer/learning_rate 0.0
wandb:    fold_5_original_transformer/train_loss 0.45635
wandb:  fold_5_original_transformer/val_accuracy 0.64444
wandb:   fold_5_original_transformer/val_auc_roc 0.61333
wandb:        fold_5_original_transformer/val_f1 0.72414
wandb:      fold_5_original_transformer/val_loss 0.44958
wandb: fold_5_original_transformer/val_precision 0.75
wandb:    fold_5_original_transformer/val_recall 0.7
wandb:         original_final/mean_best_accuracy 0.66222
wandb:               original_final/mean_best_f1 0.78375
wandb:             original_final/mean_best_loss 0.45037
wandb:          original_final/std_best_accuracy 0.01859
wandb:                original_final/std_best_f1 0.03469
wandb:              original_final/std_best_loss 0.00108
wandb:                   original_fold_1/best_f1 0.81081
wandb:         original_fold_1/best_val_accuracy 0.68889
wandb:             original_fold_1/best_val_loss 0.45147
wandb:        original_fold_1/final_val_accuracy 0.66667
wandb:            original_fold_1/final_val_loss 0.45147
wandb:                original_fold_1/pos_weight 0.5
wandb:             original_fold_1/test_accuracy 0.7193
wandb:                  original_fold_1/test_auc 0.74559
wandb:                   original_fold_1/test_f1 0.83333
wandb:                 original_fold_1/test_loss 0.42439
wandb:                  original_fold_1/train_ad 120
wandb:            original_fold_1/train_ad_ratio 0.66667
wandb:                  original_fold_1/train_cn 60
wandb:            original_fold_1/train_cn_ratio 0.33333
wandb:               original_fold_1/train_total 180
wandb:                    original_fold_1/val_ad 30
wandb:              original_fold_1/val_ad_ratio 0.66667
wandb:                    original_fold_1/val_cn 15
wandb:              original_fold_1/val_cn_ratio 0.33333
wandb:                 original_fold_1/val_total 45
wandb:                   original_fold_2/best_f1 0.8
wandb:         original_fold_2/best_val_accuracy 0.66667
wandb:             original_fold_2/best_val_loss 0.44895
wandb:        original_fold_2/final_val_accuracy 0.66667
wandb:            original_fold_2/final_val_loss 0.44895
wandb:                original_fold_2/pos_weight 0.5
wandb:             original_fold_2/test_accuracy 0.70175
wandb:                  original_fold_2/test_auc 0.75147
wandb:                   original_fold_2/test_f1 0.82474
wandb:                 original_fold_2/test_loss 0.42737
wandb:                  original_fold_2/train_ad 120
wandb:            original_fold_2/train_ad_ratio 0.66667
wandb:                  original_fold_2/train_cn 60
wandb:            original_fold_2/train_cn_ratio 0.33333
wandb:               original_fold_2/train_total 180
wandb:                    original_fold_2/val_ad 30
wandb:              original_fold_2/val_ad_ratio 0.66667
wandb:                    original_fold_2/val_cn 15
wandb:              original_fold_2/val_cn_ratio 0.33333
wandb:                 original_fold_2/val_total 45
wandb:                   original_fold_3/best_f1 0.8
wandb:         original_fold_3/best_val_accuracy 0.66667
wandb:             original_fold_3/best_val_loss 0.45059
wandb:        original_fold_3/final_val_accuracy 0.44444
wandb:            original_fold_3/final_val_loss 0.45059
wandb:                original_fold_3/pos_weight 0.5
wandb:             original_fold_3/test_accuracy 0.4386
wandb:                  original_fold_3/test_auc 0.71176
wandb:                   original_fold_3/test_f1 0.38462
wandb:                 original_fold_3/test_loss 0.43428
wandb:                  original_fold_3/train_ad 120
wandb:            original_fold_3/train_ad_ratio 0.66667
wandb:                  original_fold_3/train_cn 60
wandb:            original_fold_3/train_cn_ratio 0.33333
wandb:               original_fold_3/train_total 180
wandb:                    original_fold_3/val_ad 30
wandb:              original_fold_3/val_ad_ratio 0.66667
wandb:                    original_fold_3/val_cn 15
wandb:              original_fold_3/val_cn_ratio 0.33333
wandb:                 original_fold_3/val_total 45
wandb:                   original_fold_4/best_f1 0.78378
wandb:         original_fold_4/best_val_accuracy 0.64444
wandb:             original_fold_4/best_val_loss 0.45125
wandb:        original_fold_4/final_val_accuracy 0.46667
wandb:            original_fold_4/final_val_loss 0.45125
wandb:                original_fold_4/pos_weight 0.5
wandb:             original_fold_4/test_accuracy 0.47368
wandb:                  original_fold_4/test_auc 0.70147
wandb:                   original_fold_4/test_f1 0.44444
wandb:                 original_fold_4/test_loss 0.43404
wandb:                  original_fold_4/train_ad 120
wandb:            original_fold_4/train_ad_ratio 0.66667
wandb:                  original_fold_4/train_cn 60
wandb:            original_fold_4/train_cn_ratio 0.33333
wandb:               original_fold_4/train_total 180
wandb:                    original_fold_4/val_ad 30
wandb:              original_fold_4/val_ad_ratio 0.66667
wandb:                    original_fold_4/val_cn 15
wandb:              original_fold_4/val_cn_ratio 0.33333
wandb:                 original_fold_4/val_total 45
wandb:                   original_fold_5/best_f1 0.72414
wandb:         original_fold_5/best_val_accuracy 0.64444
wandb:             original_fold_5/best_val_loss 0.44958
wandb:        original_fold_5/final_val_accuracy 0.64444
wandb:            original_fold_5/final_val_loss 0.44958
wandb:                original_fold_5/pos_weight 0.5
wandb:             original_fold_5/test_accuracy 0.7193
wandb:                  original_fold_5/test_auc 0.73971
wandb:                   original_fold_5/test_f1 0.78947
wandb:                 original_fold_5/test_loss 0.42466
wandb:                  original_fold_5/train_ad 120
wandb:            original_fold_5/train_ad_ratio 0.66667
wandb:                  original_fold_5/train_cn 60
wandb:            original_fold_5/train_cn_ratio 0.33333
wandb:               original_fold_5/train_total 180
wandb:                    original_fold_5/val_ad 30
wandb:              original_fold_5/val_ad_ratio 0.66667
wandb:                    original_fold_5/val_cn 15
wandb:              original_fold_5/val_cn_ratio 0.33333
wandb:                 original_fold_5/val_total 45
wandb:               original_test/mean_accuracy 0.61053
wandb:                    original_test/mean_auc 0.73
wandb:                     original_test/mean_f1 0.65532
wandb:                original_test/std_accuracy 0.12671
wandb:                     original_test/std_auc 0.01972
wandb:                      original_test/std_f1 0.19806
wandb:
wandb:  View run dutiful-elevator-12 at: https://wandb.ai/yuho2074tamura1217-keio-university-global-page/silence-transformer-classification/runs/v1nqv5z6
wandb:  View project at: https://wandb.ai/yuho2074tamura1217-keio-university-global-page/silence-transformer-classification
wandb: Synced 5 W&B file(s), 7 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: .\wandb\run-20250903_190949-v1nqv5z6\logs
(venv) PS C:\Users\yuho2\CogniAlign\modules> 